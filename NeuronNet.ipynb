{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNet(object):\n",
    "    # yi = Wi * xi + bi\n",
    "    def __init__(self, sizes):\n",
    "        # Number of layers in the network\n",
    "        self.num_layers = len(sizes)\n",
    "        # Number of neuron in the network\n",
    "        self.num_neuron = sizes\n",
    "        # Populate Gaussian random of bias vectors, layer by layer\n",
    "        self.bs = [ np.random.rand(r, 1) for r in sizes[1:] ]\n",
    "        # Populate Gaussian random of weight matrix, layer by layer\n",
    "        self.Ws = [ np.random.rand(r, c) for r, c in zip(sizes[1:], sizes[:-1]) ]\n",
    "    \n",
    "    # Function sigmoid neutron\n",
    "        # Input: weighted input vector\n",
    "        # Output: normalized value of weighted input\n",
    "    def sigma(z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Function derivative of sigmoid neutron\n",
    "        # Input: sigmoid neutron z (normalized )\n",
    "        # Output: rate of change in sigmoid neutron z\n",
    "    def sigma_rate(self, z):\n",
    "        return sigma(z) * (1 - sigma(z))\n",
    "    \n",
    "    # Function feedfordward:\n",
    "        # Input: activation vector a\n",
    "        # Output: activation vector for the next layer (forward activation)    \n",
    "    def feedforward(self, a):\n",
    "        for b, W in zip(self.bs, self.Ws):\n",
    "            # Compute weighted input\n",
    "            a = sigma(np.dot(W, a) + b)\n",
    "        return a\n",
    "   \n",
    "    # Function backpropagation\n",
    "        # Input: a data sample x, y\n",
    "        # Output: a tuple of (gd_bs, gd_Ws) representing the gradient for the loss function\n",
    "        # gd_bs, with same dimension to bs', is list of bias vectors, layer by layer\n",
    "        # gd_Ws, with same dimension to Ws', is list of weight matrices, layer by layer\n",
    "    def backpropagation(self, x, y):\n",
    "        # Populate vectors in gd_bs with 0 layer by layer\n",
    "        gd_bs = [ np.zeros(b.shape) for b in self.bs ]\n",
    "        # Populate matrices in gd_Ws with 0 layer by layer\n",
    "        gd_WS = [ np.zeros(W.shape) for W in self.Ws ]\n",
    "        \n",
    "        # Feedforward\n",
    "        a = x               # input vector (the 1st layer)\n",
    "        activations = [x]   # list of all activation vectors from the 1st to the last layers\n",
    "        zs = []             # list of all weighted input vectors from the 2nd to the last layers\n",
    "        \n",
    "        for b, W in zip(self.bs, self.Ws):\n",
    "            # Compute the individual weighted input vector, layer by layer, then save in zs\n",
    "            z = np.dot(W, activation) + b\n",
    "            zs.append(z)\n",
    "            # Compute the forward activation a, layer by layer, then save in activations\n",
    "            a = sigma(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        # Back propagation\n",
    "        delta = (activations[-1] - y) * sigma_rate(zs[-1])\n",
    "        gd_bs[-1], gd_Ws[-1]= delta, np.dot(delta, activations[-2].transposes())\n",
    "        for k in range(2, self.num_layers):\n",
    "            z, s = zs[-k], sigma_rate(z)\n",
    "            delta = np.dot(self.Ws[-k + 1].transpose(), delta) * s\n",
    "            gd_bs[-k], gd_Ws[-k] = delta, np.dot(delta, activations[-k - 1].transposes())\n",
    "        \n",
    "        return (gd_bs, gd_Ws)\n",
    "        \n",
    "    # Function evaluate:\n",
    "        # Input: test data in tuple of (x, y) \n",
    "        # Output: number of correct predictions\n",
    "    def evaluate(self, test):\n",
    "        results = [ (np.argmax(self.forwardfeed(x)), y) for (x, y) in test ]\n",
    "        return sum(int(y0 == y1) for (y0, y1) in results)\n",
    "    \n",
    "    # Function update_para\n",
    "        # Input: a batch of mini samples mini_batch, and learning rate eta\n",
    "        # Output: None, just update the network's bias vectors bs and the weight matrix Ws,\n",
    "        # layer by layer using gradient descent and backpropagation algorithm \n",
    "        # applied to the mini batch with following formulars:\n",
    "        # new W = current W - eta * change in loss function per change in weight\n",
    "        # new b = current b - eta * change in loss function per change in weight\n",
    "    def update_para(self, mini_batch, eta):\n",
    "        # Populate vectors in gd_bs with 0 layer by layer\n",
    "        gd_bs = [ np.zeros(b.shape) for b in self.bs ]\n",
    "        # Populate matrices in gd_Ws with 0 layer by layer\n",
    "        gd_WS = [ np.zeros(W.shape) for W in self.Ws ]\n",
    "        for x, y in mini_batch:\n",
    "            # Compute delta bias bs and delta weights Ws\n",
    "            dt_bs, dt_Ws = backpropagation(x, y)\n",
    "            # Update vectors of gradient in bias for the loss function\n",
    "            gd_bs = [ gd_b + dt_b for gd_b, dt_b in zip(gd_bs, dt_bs)]\n",
    "            # Update matrices of gradient in weight for the loss function\n",
    "            gd_Ws = [ gd_W + dt_W for gd_W, dt_W in zip(gd_Ws, dt_Ws) ]\n",
    "        \n",
    "        # Update bias vectors in the network, layer by layer\n",
    "        self.bs = [b - (eta / len(mini_batch)) * gd_b for b, gd_b in zip(self.bs, gd_bs)]\n",
    "        # Update weight matrices in the network, layer by layer\n",
    "        self.Ws = [W - (eta / len(mini_batch)) * gd_W for W, gd_W in zip(self.Ws, gd_Ws)]\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DeepNet([4,5,4,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.82003933],\n",
       "        [0.78953694],\n",
       "        [0.04211883],\n",
       "        [0.22198753],\n",
       "        [0.93319019]]),\n",
       " array([[0.51021025],\n",
       "        [0.06248863],\n",
       "        [0.66052186],\n",
       "        [0.55094786]]),\n",
       " array([[0.8835669 ],\n",
       "        [0.78567719],\n",
       "        [0.29393298]]),\n",
       " array([[0.5719593]])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.08113222, 0.88901792, 0.08917793, 0.52732948],\n",
       "        [0.57057364, 0.89866055, 0.8649722 , 0.07670039],\n",
       "        [0.47706599, 0.89996795, 0.36966847, 0.01682053],\n",
       "        [0.94455757, 0.19365979, 0.38129643, 0.28483987],\n",
       "        [0.52039522, 0.11143282, 0.15919477, 0.25961113]]),\n",
       " array([[0.13246825, 0.62444603, 0.0316855 , 0.35444947, 0.75866716],\n",
       "        [0.63630608, 0.59366771, 0.34180139, 0.97517483, 0.71122407],\n",
       "        [0.84166072, 0.69149115, 0.61316299, 0.28953297, 0.69593004],\n",
       "        [0.23414129, 0.11782254, 0.40530646, 0.56770224, 0.73024333]]),\n",
       " array([[0.85214822, 0.3157396 , 0.211461  , 0.46292401],\n",
       "        [0.32080462, 0.82897685, 0.64763325, 0.3779733 ],\n",
       "        [0.93957912, 0.55148697, 0.12178442, 0.84041606]]),\n",
       " array([[0.69774941, 0.19504526, 0.18102969]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
