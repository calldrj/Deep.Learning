{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeepNet(object):\n",
    "    # yi = Wi * xi + bi\n",
    "    def __init__(self, sizes):\n",
    "        # Number of layers in the network\n",
    "        self.num_layers = len(sizes)\n",
    "        # Number of neuron in the network\n",
    "        self.num_neuron = sizes\n",
    "        # Populate Gaussian random of bias vectors, layer by layer\n",
    "        self.bs = [ np.random.rand(r, 1) for r in sizes[1:] ]\n",
    "        # Populate Gaussian random of weight matrix, layer by layer\n",
    "        self.Ws = [ np.random.rand(r, c) for r, c in zip(sizes[1:], sizes[:-1]) ]\n",
    "    \n",
    "    # Function computes the sigmoid neutron\n",
    "        # Input: weighted input vector of a layer\n",
    "        # Output: normalized value of weighted input vector of the same layer\n",
    "    def sigma(z):\n",
    "        return 1.0 / (1 + np.exp(-z))\n",
    "    \n",
    "    # Function computes the derivative of sigmoid neutron\n",
    "        # Input: sigmoid neutron z (normalized )\n",
    "        # Output: rate of change in sigmoid neutron z\n",
    "    def sigma_rate(self, z):\n",
    "        return sigma(z) * (1 - sigma(z))\n",
    "    \n",
    "    # Function computes the fordward activation of a layer:\n",
    "        # Input: activation vector a of a current layer\n",
    "        # Output: activation vector for the next layer (forward activation)    \n",
    "    def feedforward(self, a):\n",
    "        for b, W in zip(self.bs, self.Ws):\n",
    "            # Compute weighted input\n",
    "            a = sigma(np.dot(W, a) + b)\n",
    "        return a\n",
    "   \n",
    "    # Function backpropagation\n",
    "        # Input: a data sample x, y\n",
    "        # Output: a tuple of (gd_bs, gd_Ws) representing the gradient for the loss function\n",
    "        # gd_bs, with same dimension to bs', is list of bias vectors, layer by layer\n",
    "        # gd_Ws, with same dimension to Ws', is list of weight matrices, layer by layer\n",
    "    def backpropagation(self, x, y):\n",
    "        # Populate vectors in gd_bs with 0 layer by layer\n",
    "        gd_bs = [ np.zeros(b.shape) for b in self.bs ]\n",
    "        # Populate matrices in gd_Ws with 0 layer by layer\n",
    "        gd_WS = [ np.zeros(W.shape) for W in self.Ws ]\n",
    "        \n",
    "        # Feedforward\n",
    "        a = x               # input vector (the 1st layer)\n",
    "        activations = [x]   # list of all activation vectors from the 1st to the last layers\n",
    "        zs = []             # list of all weighted input vectors from the 2nd to the last layers\n",
    "        \n",
    "        for b, W in zip(self.bs, self.Ws):\n",
    "            # Compute the individual weighted input vector, layer by layer, then save in zs\n",
    "            z = np.dot(W, activation) + b\n",
    "            zs.append(z)\n",
    "            # Compute the forward activation a, layer by layer, then save in activations\n",
    "            a = sigma(z)\n",
    "            activations.append(a)\n",
    "        \n",
    "        # Back propagation\n",
    "        delta = (activations[-1] - y) * sigma_rate(zs[-1])\n",
    "        gd_bs[-1], gd_Ws[-1]= delta, np.dot(delta, activations[-2].transposes())\n",
    "        for k in range(2, self.num_layers):\n",
    "            z, s = zs[-k], sigma_rate(z)\n",
    "            delta = np.dot(self.Ws[-k + 1].transpose(), delta) * s\n",
    "            gd_bs[-k], gd_Ws[-k] = delta, np.dot(delta, activations[-k - 1].transposes())\n",
    "        \n",
    "        return (gd_bs, gd_Ws)\n",
    "        \n",
    "    # Function evaluate:\n",
    "        # Input: test data in tuple of (x, y) \n",
    "        # Output: number of correct predictions\n",
    "    def evaluate(self, test):\n",
    "        results = [ (np.argmax(self.forwardfeed(x)), y) for (x, y) in test ]\n",
    "        return sum(int(y0 == y1) for (y0, y1) in results)\n",
    "    \n",
    "    # Function update_para\n",
    "        # Input: a batch of mini samples mini_batch, and learning rate eta\n",
    "        # Output: None, just update the network's bias vectors bs and the weight matrix Ws,\n",
    "        # layer by layer using gradient descent and backpropagation algorithm \n",
    "        # applied to the mini batch with following formulars:\n",
    "        # new W = current W - eta * change in loss function per change in weight\n",
    "        # new b = current b - eta * change in loss function per change in weight\n",
    "    def update_para(self, mini_batch, eta):\n",
    "        # Populate vectors in gd_bs with 0 layer by layer\n",
    "        gd_bs = [ np.zeros(b.shape) for b in self.bs ]\n",
    "        # Populate matrices in gd_Ws with 0 layer by layer\n",
    "        gd_WS = [ np.zeros(W.shape) for W in self.Ws ]\n",
    "        for x, y in mini_batch:\n",
    "            # Compute delta bias bs and delta weights Ws\n",
    "            dt_bs, dt_Ws = backpropagation(x, y)\n",
    "            # Update vectors of gradient in bias for the loss function\n",
    "            gd_bs = [ gd_b + dt_b for gd_b, dt_b in zip(gd_bs, dt_bs) ]\n",
    "            # Update matrices of gradient in weight for the loss function\n",
    "            gd_Ws = [ gd_W + dt_W for gd_W, dt_W in zip(gd_Ws, dt_Ws) ]\n",
    "        \n",
    "        # Update bias vectors in the network, layer by layer\n",
    "        self.bs = [ b - (eta / len(mini_batch)) * gd_b for b, gd_b in zip(self.bs, gd_bs) ]\n",
    "        # Update weight matrices in the network, layer by layer\n",
    "        self.Ws = [ W - (eta / len(mini_batch)) * gd_W for W, gd_W in zip(self.Ws, gd_Ws) ]\n",
    "    \n",
    "    # Function Stochastic Gradient Descent (SGD)\n",
    "    # Input: training dataset, number of epochs, size of mini batch, learning rate, \n",
    "    # ooptional testing dataset\n",
    "    # Output: None, just print out the training progress per every epoch\n",
    "    def SGD(train_dataset, epochs, batch_size, eta, test_dataset=None):\n",
    "        for k in range(epochs):\n",
    "            random.shuffle(train_dataset)     # randomize samples in training data\n",
    "            batch = [ train_dataset[k : batch_size]\n",
    "                      for k in range(0, len(train_dataset), batch_size) ]\n",
    "            for sample in batch:\n",
    "                self.update_para(mini_batch, eta)\n",
    "            if test_dataset:\n",
    "                print (\"Epoch {0}: {1} / {2} ...\".format(k, \n",
    "                                                         self.evaluate(test_dataset), \n",
    "                                                         len(test_dataset)))\n",
    "            else:\n",
    "                print (\"Epoch {0} complete ...\".format(k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import cPickle\n",
    "import gzip\n",
    "\n",
    "# Function load raw dataset\n",
    "def load_dataset():\n",
    "    file = gzip.open('../data/mnist.pkl.gz', 'rb')\n",
    "    train_data, val_data, test_data = cPickle.load(file)\n",
    "    file.close()\n",
    "    return (train_data, val_data, test_data)\n",
    "\n",
    "# Function vectorize ouput y\n",
    "def vector_y(k):\n",
    "    y = np.zeros((10, 1))\n",
    "    y[k] = 1.0\n",
    "    return y \n",
    "\n",
    "# Function process and split raw dataset\n",
    "def wrapper_data():\n",
    "    train_set, val_set, test_set = load_dataset()\n",
    "    # Create training dataset\n",
    "    train_x = [ np.reshape(x, (784, 1) for x in train_set[0]) ]\n",
    "    train_y = [ vector_y(y) for y in train_set[1] ]\n",
    "    train_dataset = zip(train_x, train_y)\n",
    "    # Create validation dataset\n",
    "    val_x = [ np.reshape(x, (784, 1) for x in val_set[0]) ]\n",
    "    val_dataset = zip(val_x, val_set[1])\n",
    "    # Create testing dataset\n",
    "    test_x = [ np.reshape(x, (784, 1) for x in test_set[0]) ]\n",
    "    test_dataset = zip(test_x, test_set[1])\n",
    "    return (train_dataset, val_dataset, test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.77208022],\n",
       "        [0.58771298],\n",
       "        [0.24699668],\n",
       "        [0.92510029],\n",
       "        [0.19689734]]),\n",
       " array([[0.28579025],\n",
       "        [0.43985276],\n",
       "        [0.69786919],\n",
       "        [0.39248504]]),\n",
       " array([[0.71354171],\n",
       "        [0.10932937],\n",
       "        [0.58095648]]),\n",
       " array([[0.76726167]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.15775911, 0.96312765, 0.11809137, 0.78549728],\n",
       "        [0.09585281, 0.38056822, 0.73509153, 0.57089797],\n",
       "        [0.75125599, 0.65372063, 0.8362623 , 0.83877212],\n",
       "        [0.41280483, 0.46687127, 0.57205302, 0.39789023],\n",
       "        [0.0676038 , 0.94905966, 0.74833469, 0.22780325]]),\n",
       " array([[0.5594655 , 0.41872864, 0.07774587, 0.90815845, 0.46914133],\n",
       "        [0.91804469, 0.31938242, 0.61422851, 0.88599827, 0.76309882],\n",
       "        [0.68787645, 0.72778859, 0.0393671 , 0.82094275, 0.98610901],\n",
       "        [0.04088035, 0.21916091, 0.68797404, 0.16469924, 0.63102303]]),\n",
       " array([[0.32271802, 0.97811405, 0.73673734, 0.96879185],\n",
       "        [0.07049792, 0.30469496, 0.15983859, 0.13077629],\n",
       "        [0.97174731, 0.25108595, 0.47372285, 0.50158101]]),\n",
       " array([[0.42448854, 0.3675324 , 0.34832972]])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
